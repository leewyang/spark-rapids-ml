{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c21330-7326-4d98-9351-d1b2e4c6143c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "291da378-0e9b-4b53-bf9e-b78c35631f1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_rows = 100000\n",
    "n_cols = 500\n",
    "n_clusters_data = 200\n",
    "cluster_std = 1.0\n",
    "dtype='float32'\n",
    "from cuml.datasets import make_blobs\n",
    "data, _ = make_blobs(\n",
    "        n_rows, n_cols, n_clusters_data, cluster_std=cluster_std, random_state=0, dtype=dtype\n",
    "    )  # make_blobs creates a random dataset of isotropic gaussian blobs.\n",
    "\n",
    "data = data.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ae1eaf-a6d7-467d-88c0-644ae814c488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Convert dataset to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ae1eaf-a6d7-467d-88c0-644ae814c488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_data = pd.DataFrame({\"features\": list(data)})\n",
    "df = spark.createDataFrame(pd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use this function to build both the Spark RAPIDS ML (GPU) and Spark ML (CPU) linear estimator objects, demonstrating the common API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kmeans_estimator(estimator_class):\n",
    "    return ( \n",
    "            estimator_class()\n",
    "            .setTol(1.0e-20)\n",
    "            .setK(200)\n",
    "            .setFeaturesCol(\"features\")\n",
    "            .setMaxIter(15)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b36ddc-2f90-409b-a213-3f319c736134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark RAPIDS ML (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1f994f0-4ca6-4b63-88f7-b0ac94ee3130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import KMeans\n",
    "gpu_kmeans = build_kmeans_estimator(KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a2cac5-18cd-450a-8571-195be588a361",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Estimator can be persisted and reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_path = \"/tmp/kmeans-estimator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "798f5b5b-cfa6-45e4-aa18-40c0142e894a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_kmeans.write().overwrite().save(estimator_path)\n",
    "gpu_kmeans_loaded = KMeans.load(estimator_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "209d8f9c-1d1c-4bdd-880c-10e46f9d0c49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "gpu_model = gpu_kmeans_loaded.fit(df)\n",
    "gpu_fit = time.time() - start_time\n",
    "print(f\"Fit took: {gpu_fit} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77499eff-5cf2-4ce6-95a1-e45b69abe3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_kmeans_loaded.getK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1abf6fbd-c7bd-400e-80a4-83284865535c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted_clusters = sorted([vec.tolist() for vec in gpu_model.clusterCenters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db84a983-a64f-462b-813b-7b7ba629d18e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[vec[0:10] for vec in sorted_clusters[0:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/tmp/kmeans-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6874b600-707d-48a7-b780-5d4e939a746b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441c8635-9070-426f-8626-7083f34b7e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpu_model_loaded = gpu_model.read().load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f7975c-f2ce-471d-aa8e-8dc678a44f37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[vec[0:10] for vec in sorted(gpu_model_loaded.cluster_centers_)[0:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3bfc179-e702-4198-8122-3a8ba98113a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = gpu_model_loaded.setPredictionCol(\"transformed\").transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0ce32d-3d09-4e82-b6a7-26978925181a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c63b707-0e00-4961-8a76-82f347886b83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c5d3b5-ecb0-435a-8976-bc18a28e3e04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccc646a3-6063-42f4-8909-03c285ce55d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark ML (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "122a7ab9-d142-4244-b79c-bbf7e6eb05e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "cpu_kmeans = build_kmeans_estimator(KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd173dd2-7058-4d73-a969-c20e9f55e3e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Convert array sql type to VectorUDT Dataframe expected by Spark ML algos (Note: Spark RAPIDS ML also accepts VectorUDT Dataframes in addition to array type Dataframe above, along with a scalar column format - see docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8a18da-17a5-4fcc-b2a5-604ea1088e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import array_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9882dae-8c66-4e4d-8164-36040e2f0f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vector_df = df.select(array_to_vector(\"features\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a48fdc86-e827-4545-a923-109519c675d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "cpu_kmeans_model = cpu_kmeans.fit(vector_df)\n",
    "cpu_fit = time.time() - start_time\n",
    "print(f\"Fit took: {cpu_fit} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb9b7ea-15db-4678-8087-40b69c13c7bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted_cpu_cluster_centers = sorted([vec.tolist() for vec in cpu_kmeans_model.clusterCenters()])\n",
    "[vec[0:10] for vec in sorted_cpu_cluster_centers[0:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec2a3245-fa44-4b08-b4f0-dc9c16b1528e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_transformed = cpu_kmeans_model.setPredictionCol(\"transformed\").transform(vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9afcab13-c69c-4a2d-a98f-5a80345d65eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_transformed.filter(spark_transformed.transformed >= 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c02f6f1-36ac-4a02-995f-6ad68ee38cec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_transformed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: CPU MinMaxScaler + GPU KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: cuML has a [MinMaxScaler](https://docs.rapids.ai/api/cuml/nightly/api.html#cuml.preprocessing.MinMaxScaler), but it needs to be exposed as a [Spark ML Transformer](https://spark.apache.org/docs/latest/ml-pipeline.html#transformers) (not sure of potential impact on performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "from spark_rapids_ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features')\n",
    "gpu_kmeans = (\n",
    "    KMeans()\n",
    "    .setTol(1.0e-20)\n",
    "    .setK(200)\n",
    "    .setFeaturesCol(\"scaled_features\")\n",
    "    .setPredictionCol(\"transformed\")\n",
    "    .setMaxIter(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[scaler, gpu_kmeans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pipe_model = pipe.fit(vector_df)\n",
    "cpu_gpu_pipe = time.time() - start_time\n",
    "print(f\"Fit took: {cpu_gpu_pipe} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_transformed = pipe_model.transform(vector_df)\n",
    "pipe_transformed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: CPU MinMaxScaler + CPU KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features')\n",
    "cpu_kmeans = (\n",
    "    KMeans()\n",
    "    .setTol(1.0e-20)\n",
    "    .setK(200)\n",
    "    .setFeaturesCol(\"scaled_features\")\n",
    "    .setPredictionCol(\"transformed\")\n",
    "    .setMaxIter(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[scaler, cpu_kmeans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pipe_model = pipe.fit(vector_df)\n",
    "cpu_cpu_pipe = time.time() - start_time\n",
    "print(f\"Fit took: {cpu_cpu_pipe} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_transformed = pipe_model.transform(vector_df)\n",
    "pipe_transformed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Pipeline (numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c21330-7326-4d98-9351-d1b2e4c6143c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "291da378-0e9b-4b53-bf9e-b78c35631f1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_rows = 10000\n",
    "n_cols = 500\n",
    "n_clusters_data = 200\n",
    "cluster_std = 1.0\n",
    "dtype='float32'\n",
    "from cuml.datasets import make_blobs\n",
    "data, _ = make_blobs(\n",
    "        n_rows, n_cols, n_clusters_data, cluster_std=cluster_std, random_state=0, dtype=dtype\n",
    "    )  # make_blobs creates a random dataset of isotropic gaussian blobs.\n",
    "\n",
    "data = data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ae1eaf-a6d7-467d-88c0-644ae814c488",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/26 10:00:41 WARN TaskSetManager: Stage 0 contains a task of very large size (1298 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data = pd.DataFrame({\"features\": list(data)})\n",
    "df = spark.createDataFrame(pd_data).repartition(2).cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('features', ArrayType(FloatType(), True), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kmeans_estimator(estimator_class):\n",
    "    return ( \n",
    "            estimator_class()\n",
    "            .setTol(1.0e-20)\n",
    "            .setK(200)\n",
    "            .setFeaturesCol(\"features\")\n",
    "            .setMaxIter(15)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_kmeans = (\n",
    "    KMeans(fake_pipe=True, use_comms=False, use_cupy=False)\n",
    "    .setTol(1.0e-20)\n",
    "    .setK(200)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"transformed\")\n",
    "    .setMaxIter(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit took: 9.797769784927368 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipe_model = gpu_kmeans.fit(df)\n",
    "fake_pipe_numpy = time.time() - start_time\n",
    "print(f\"Fit took: {fake_pipe_numpy} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fake Pipeline (cupy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_kmeans = (\n",
    "    KMeans(fake_pipe=True, use_comms=False, use_cupy=True)\n",
    "    .setTol(1.0e-20)\n",
    "    .setK(200)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"transformed\")\n",
    "    .setMaxIter(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit took: 4.041950464248657 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipe_model = gpu_kmeans.fit(df)\n",
    "fake_pipe_cupy = time.time() - start_time\n",
    "print(f\"Fit took: {fake_pipe_cupy} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Pipeline (numpy, comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_kmeans = (\n",
    "    KMeans(fake_pipe=True, use_comms=True, use_nccl=False, use_cupy=False)\n",
    "    .setTol(1.0e-20)\n",
    "    .setK(200)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"transformed\")\n",
    "    .setMaxIter(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit took: 5.000985622406006 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipe_model = gpu_kmeans.fit(df)\n",
    "fake_pipe_numpy_comms = time.time() - start_time\n",
    "print(f\"Fit took: {fake_pipe_numpy_comms} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Pipeline (cupy, comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_kmeans = (\n",
    "    KMeans(fake_pipe=True, use_comms=True, use_nccl=False, use_cupy=True)\n",
    "    .setTol(1.0e-20)\n",
    "    .setK(200)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"transformed\")\n",
    "    .setMaxIter(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit took: 4.967763185501099 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipe_model = gpu_kmeans.fit(df)\n",
    "fake_pipe_cupy_comms = time.time() - start_time\n",
    "print(f\"Fit took: {fake_pipe_cupy_comms} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Pipeline (numpy, comms, nccl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_kmeans = (\n",
    "    KMeans(fake_pipe=True, use_comms=True, use_nccl=True, use_cupy=False)\n",
    "    .setTol(1.0e-20)\n",
    "    .setK(200)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"transformed\")\n",
    "    .setMaxIter(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit took: 4.081805944442749 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipe_model = gpu_kmeans.fit(df)\n",
    "fake_pipe_numpy_nccl = time.time() - start_time\n",
    "print(f\"Fit took: {fake_pipe_numpy_nccl} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Pipeline (cupy, comms, nccl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spark_rapids_ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_kmeans = (\n",
    "    KMeans(fake_pipe=True, use_comms=True, use_nccl=True, use_cupy=True)\n",
    "    .setTol(1.0e-20)\n",
    "    .setK(200)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"transformed\")\n",
    "    .setMaxIter(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit took: 4.024225473403931 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pipe_model = gpu_kmeans.fit(df)\n",
    "fake_pipe_cupy_nccl = time.time() - start_time\n",
    "print(f\"Fit took: {fake_pipe_cupy_nccl} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(gpu_fit)\n",
    "print(cpu_fit)\n",
    "print(cpu_gpu_pipe)\n",
    "print(cpu_cpu_pipe)\n",
    "print(fake_pipe_numpy)\n",
    "print(fake_pipe_cupy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import MinMaxScaler\n",
    "import cupy as cp\n",
    "\n",
    "data = [[-1.0, 2], \n",
    "        [-0.5, 6],\n",
    "        [ 0.0, 10],\n",
    "        [ 1.0, 18]]\n",
    "\n",
    "data = cp.array(data)\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.data_max_\n",
    "# [ 1. 18.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.data_min_\n",
    "# [-1.  2.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.data_range_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.feature_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.transform(data))\n",
    "# [[0.   0.  ]\n",
    "#  [0.25 0.25]\n",
    "#  [0.5  0.5 ]\n",
    "#  [1.   1.  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.transform(cp.array([[2, 2]])))\n",
    "# [[1.5 0. ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually calculate scale_ and min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_ = (scaler.feature_range[1] - scaler.feature_range[0]) / (scaler.data_max_ - scaler.data_min_)\n",
    "scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_ = scaler.feature_range[0] - scaler.data_min_ * scale_\n",
    "min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cp.array([[20., 20.],\n",
    "                 [-20., -20.]])\n",
    "new_scaler = MinMaxScaler()\n",
    "new_scaler.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_scaler.scale_\n",
    "# array([0.025, 0.025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_scaler.min_\n",
    "# array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_scaler.transform(cp.array([[2, 2]]))\n",
    "# array([[0.55, 0.55]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.scale_ = new_scaler.scale_\n",
    "scaler.min_ = new_scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(scaler.transform(cp.array([[2, 2]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_ = (scaler.feature_range[1] - scaler.feature_range[0]) / (scaler.data_max_ - scaler.data_min_)\n",
    "scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cupy.cuda import nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nccl."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-rapids-ml-kmeans-demo",
   "notebookOrigID": 1026070411409745,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
