{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334282ff-6a6c-4603-a68a-a2af4f519c4e",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b85b6-f670-4a10-9c42-6d5369cd26f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29f1171-a4be-44e9-907a-919e7261f9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26964b6b-4686-42cc-9a5d-88bb71ca17e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b\n",
       "0  1.0  3.0\n",
       "1  2.0  2.0\n",
       "2  3.0  1.0\n",
       "3  4.0  1.0\n",
       "4  4.0  0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [[1.0, 3.0],\n",
    "        [2.0, 2.0],\n",
    "        [3.0, 1.0],\n",
    "        [4.0, 1.0],\n",
    "        [4.0, 0.0]]\n",
    "\n",
    "pdf = pd.DataFrame(data, columns=['a', 'b'])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7abf05-d772-44e5-98fe-932fb6309b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            x\n",
       "0  [1.0, 3.0]\n",
       "1  [2.0, 2.0]\n",
       "2  [3.0, 1.0]\n",
       "3  [4.0, 1.0]\n",
       "4  [4.0, 0.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_arr = pd.DataFrame()\n",
    "pdf_arr['x'] = pdf.values.tolist()\n",
    "pdf_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4355823-6cb0-4712-b35b-0936576122ac",
   "metadata": {},
   "source": [
    "### Spark DataFrame (Scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef41a8a-4b28-4e0a-9873-e3fe3df0163f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|1.0|3.0|\n",
      "|2.0|2.0|\n",
      "|3.0|1.0|\n",
      "|4.0|1.0|\n",
      "|4.0|0.0|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- a: double (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.show(); df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61daa91-8871-460b-a110-b9e735a975a9",
   "metadata": {},
   "source": [
    "### Spark DataFrame (Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc7de94-d26c-40ac-9a71-b4a698d7cca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|         x|\n",
      "+----------+\n",
      "|[1.0, 3.0]|\n",
      "|[2.0, 2.0]|\n",
      "|[3.0, 1.0]|\n",
      "|[4.0, 1.0]|\n",
      "|[4.0, 0.0]|\n",
      "+----------+\n",
      "\n",
      "root\n",
      " |-- x: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr = spark.createDataFrame(pdf_arr)\n",
    "df_arr.show(); df_arr.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d81af-e898-4dd6-b607-9f0e0ea03d4e",
   "metadata": {},
   "source": [
    "### Spark DataFrame (Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8e2b93-d869-4494-86b1-e974bc657764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|        x|\n",
      "+---------+\n",
      "|[1.0,3.0]|\n",
      "|[2.0,2.0]|\n",
      "|[3.0,1.0]|\n",
      "|[4.0,1.0]|\n",
      "|[4.0,0.0]|\n",
      "+---------+\n",
      "\n",
      "root\n",
      " |-- x: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "df_vec = df_arr.select(array_to_vector(\"x\").alias(\"x\"))\n",
    "df_vec.show(); df_vec.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc71805-23e1-4379-a8e8-a25de24fe6c8",
   "metadata": {},
   "source": [
    "## Max (Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac2d4a7-ee7d-45c6-92c5-e0ede6c5644f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    max(x)|\n",
      "+----------+\n",
      "|[4.0, 1.0]|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_arr.select(max(df_arr.x)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b466fa-8c8e-4c18-a59b-3b8c2e6013f0",
   "metadata": {},
   "source": [
    "## Max (Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7a0398-e08a-48f1-a681-706fbc20adf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   max(x)|\n",
      "+---------+\n",
      "|[4.0,1.0]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vec.select(max(df_vec.x)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da966b1d-cf1a-4169-83ce-ee97e0a6928c",
   "metadata": {},
   "source": [
    "## Summarizer (Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69606376-9560-42fe-9b25-d94e968d5171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.ml.stat import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5627eb-52b4-4381-9043-3d3ee04f9a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = Summarizer.metrics(\"min\", \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae4ec73b-a6f0-48d6-96f0-4e4d56f37921",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"aggregate_metrics(x, 1.0)\" due to data type mismatch: Parameter 1 requires the \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\" type, however \"x\" has the type \"ARRAY<DOUBLE>\".;\n'Aggregate [unresolvedalias(aggregate_metrics(Min, Max, ComputeMax, ComputeMin, ComputeNNZ, x#13, 1.0, 0, 0), Some(org.apache.spark.sql.Column$$Lambda$1525/491152434@71eb0e3d))]\n+- LocalRelation [x#13]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_arr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummarizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_arr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:3089\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3045\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3046\u001b[0m \n\u001b[1;32m   3047\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3088\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3089\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/errors/exceptions/captured.py:182\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"aggregate_metrics(x, 1.0)\" due to data type mismatch: Parameter 1 requires the \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\" type, however \"x\" has the type \"ARRAY<DOUBLE>\".;\n'Aggregate [unresolvedalias(aggregate_metrics(Min, Max, ComputeMax, ComputeMin, ComputeNNZ, x#13, 1.0, 0, 0), Some(org.apache.spark.sql.Column$$Lambda$1525/491152434@71eb0e3d))]\n+- LocalRelation [x#13]\n"
     ]
    }
   ],
   "source": [
    "df_arr.select(summarizer.summary(df_arr.x)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d3af2-3f6b-4a34-9277-acebbb374c40",
   "metadata": {},
   "source": [
    "## Summarizer (Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef0418dc-9bf8-4aa3-8463-9d275d2dd2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|aggregate_metrics(x, 1.0)|\n",
      "+-------------------------+\n",
      "|   {[1.0,0.0], [4.0,3.0]}|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vec.select(summarizer.summary(df_vec.x)).show(truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43bd048-76c1-45f2-b37e-2bec4ed231b9",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b93e06-2ccb-428b-9839-83a2e7b086e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_vec.createOrReplaceTempView(\"tmpview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acab947a-bf7b-4e0c-abee-eee200411dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   max(x)|\n",
      "+---------+\n",
      "|[4.0,1.0]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(x) from tmpview\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae71759-af81-4c49-a8bc-cb3c29e14571",
   "metadata": {},
   "source": [
    "## Scalar Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0660073c-e049-4ffc-9f2f-b1428cbae908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68faaaf7-72f8-438e-911e-b3c9d36594ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n",
      "|min(a)|min(b)|max(a)|max(b)|\n",
      "+------+------+------+------+\n",
      "|   1.0|   0.0|   4.0|   3.0|\n",
      "+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(\"a\"), min(\"b\"), max(\"a\"), max(\"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1f184-d104-405b-983e-01ffe22ffc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
